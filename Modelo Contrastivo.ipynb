{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Modelos Contrastivos (CLIP-like)\n",
    "Entrena un modelo basado en contraste, donde el objetivo es minimizar la distancia entre embeddings de texto humano y maximizar la distancia entre humano y generado por IA.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "Permite aprender representaciones robustas.\n",
    "Se puede usar junto con un clasificador simple para la predicción final.\n",
    "Ejemplo de entrenamiento contrastivo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 256\n",
    "LAYERS_TO_TRAIN = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_generated_path = \"pan24-generative-authorship-news/machines\"\n",
    "human_path = \"pan24-generative-authorship-news/human.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\".*overflowing tokens.*\")\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, id, text = [], [], []\n",
    "\n",
    "# Loop through every file in the directory\n",
    "for filename in os.listdir(ai_generated_path):\n",
    "    # Check if the file is a JSONL file\n",
    "    if filename.endswith('.jsonl'):\n",
    "        filepath = os.path.join(ai_generated_path, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as jsonl_file:\n",
    "            for line in jsonl_file:\n",
    "                # Each line is a separate JSON object\n",
    "                data = json.loads(line)\n",
    "                model.append(filename)\n",
    "                id.append(data['id'])\n",
    "                text.append(data['text'])\n",
    "\n",
    "df_generated = pd.DataFrame({'model': model, 'id': id, 'text': text, 'ai_generated': 1})\n",
    "df_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id, text = [], []\n",
    "\n",
    "with open(human_path, 'r', encoding='utf-8') as jsonl_file:\n",
    "    for line in jsonl_file:\n",
    "        # Each line is a separate JSON object\n",
    "        data = json.loads(line)\n",
    "        id.append(data['id'])\n",
    "        text.append(data['text'])\n",
    "\n",
    "df_human = pd.DataFrame({'model': 'Human', 'id': id, 'text': text, 'ai_generated': 0})\n",
    "df_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_generated, df_human])[['text', 'ai_generated', 'id']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data - Combinaciones únicamente del mismo id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "test_size = 0.25\n",
    "val_size = 0.125\n",
    "_adjusted_val_size = val_size / (1 - test_size)\n",
    "\n",
    "# Extraer el segundo y tercer segmento de los IDs\n",
    "df['base_id'] = df['id'].apply(lambda x: '/'.join(x.split('/')[1:]))  # Coger los ids sin la parte que identifica al autor del fragmento de texto.\n",
    "\n",
    "# Paso 1: Dividir los datos según los `base_id`\n",
    "base_ids = df['base_id'].unique()\n",
    "train_base_ids, test_base_ids = train_test_split(base_ids, test_size=test_size, random_state=1337)\n",
    "train_base_ids, val_base_ids = train_test_split(train_base_ids, test_size=_adjusted_val_size, random_state=1337) \n",
    "\n",
    "# Crear DataFrames por conjunto\n",
    "train = df[df['base_id'].isin(train_base_ids)]\n",
    "val = df[df['base_id'].isin(val_base_ids)]\n",
    "test = df[df['base_id'].isin(test_base_ids)]\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"train shape: {train.shape}\")\n",
    "print(f\"val shape: {val.shape}\")\n",
    "print(f\"test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combinations_within_id(df):\n",
    "    # Lista para almacenar las combinaciones de cada `base_id`\n",
    "    combinations = []\n",
    "    \n",
    "    # Iterar sobre cada `base_id`\n",
    "    for base_id, group in df.groupby('base_id'):\n",
    "        # Filtrar textos humanos e IA dentro del grupo\n",
    "        df_human = group[group['ai_generated'] == 0][['text']].reset_index(drop=True)\n",
    "        df_ia = group[group['ai_generated'] == 1][['text']].reset_index(drop=True)\n",
    "        \n",
    "        # Producto cartesiano dentro del `base_id`\n",
    "        cartesian_df = df_human.merge(df_ia, how='cross', suffixes=('_human', '_ia'))\n",
    "        cartesian_df = cartesian_df.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "        # Crear las dos disposiciones\n",
    "        total_combinations = len(cartesian_df)\n",
    "        \n",
    "        half_1 = cartesian_df.iloc[:total_combinations // 2].copy()\n",
    "        half_1['comment_text_1'] = half_1['text_human']\n",
    "        half_1['comment_text_2'] = half_1['text_ia']\n",
    "        half_1['list'] = 0  # Etiqueta 0\n",
    "        \n",
    "        half_2 = cartesian_df.iloc[total_combinations // 2:].copy()\n",
    "        half_2['comment_text_1'] = half_2['text_ia']\n",
    "        half_2['comment_text_2'] = half_2['text_human']\n",
    "        half_2['list'] = 1  # Etiqueta 1\n",
    "        \n",
    "        # Combinar y agregar al resultado final\n",
    "        balanced_df = pd.concat([half_1, half_2], ignore_index=True)\n",
    "        combinations.append(balanced_df)\n",
    "    \n",
    "    # Concatenar todas las combinaciones y barajar\n",
    "    return pd.concat(combinations, ignore_index=True).sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar combinaciones restringidas por `id` para cada conjunto\n",
    "train = create_combinations_within_id(train)\n",
    "val = create_combinations_within_id(val)\n",
    "test = create_combinations_within_id(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dimensions\n",
    "print(f\"train shape: {train.shape} / Text on comment_text_1 is human-generated: {train['list'].value_counts()[0]} - Text on comment_text_2 is human-generated: {train['list'].value_counts()[1]}\")\n",
    "print(f\"val shape: {val.shape} / Text on comment_text_1 is human-generated: {val['list'].value_counts()[0]} - Text on comment_text_2 is human-generated: {val['list'].value_counts()[1]}\")\n",
    "print(f\"test shape: {test.shape} / Text on comment_text_1 is human-generated: {test['list'].value_counts()[0]} - Text on comment_text_2 is human-generated: {test['list'].value_counts()[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Tokenizador y modelo\n",
    "model_name = \"Lau123/distilbert-base-uncased-detect_ai_generated_text\"\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "# individual_model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "individual_model =  BertModel.from_pretrained(model_name).from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Configuración del dispositivo y optimizador\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# individual_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeze all layers except the classifier layer\n",
    "# for name, param in individual_model.named_parameters():\n",
    "#     if name != \"classifier.weight\" and name != \"classifier.bias\":\n",
    "#         param.requires_grad = False\n",
    "\n",
    "for param in individual_model.parameters():\n",
    "    param.requires_grad = False\n",
    "if LAYERS_TO_TRAIN > 0:\n",
    "    for layer in individual_model.encoder.layer[-LAYERS_TO_TRAIN:]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "for param in individual_model.pooler.dense.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Verify that only the classifier layer is trainable\n",
    "for name, param in individual_model.named_parameters():\n",
    "    print(f\"{name}: requires_grad = {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def contrastive_loss(embeddings1, embeddings2, labels, margin=0.5):\n",
    "    sim = cosine_similarity(embeddings1, embeddings2)\n",
    "    loss = torch.mean(labels * (1 - sim) + (1 - labels) * torch.clamp(sim - margin, min=0))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerContrastive(torch.nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(TransformerContrastive, self).__init__()\n",
    "        self.l1 = bert_model  # Modelo BERT o similar\n",
    "        self.l2 = torch.nn.Linear(768, 768)  # Proyección del embedding\n",
    "        self.l3 = torch.nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, ids_0, mask_0, token_type_ids_0, ids_1, mask_1, token_type_ids_1):\n",
    "        # Generar embeddings para ambos textos\n",
    "        embed_a = self.l1(ids_0, attention_mask=mask_0, token_type_ids=token_type_ids_0).last_hidden_state[:, 0]\n",
    "        embed_b = self.l1(ids_1, attention_mask=mask_1, token_type_ids=token_type_ids_1).last_hidden_state[:, 0]\n",
    "\n",
    "        # Proyección a un espacio latente (opcional, pero puede mejorar resultados)\n",
    "        embed_a = F.gelu(self.l3(self.l2(embed_a)))\n",
    "        embed_b = F.gelu(self.l3(self.l2(embed_b)))\n",
    "\n",
    "        return embed_a, embed_b  # Devuelve los embeddings de ambos textos\n",
    "\n",
    "# Inicializar modelo\n",
    "model = TransformerContrastive(individual_model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, embed_a, embed_b, label):\n",
    "        # Distancia euclidiana entre embeddings\n",
    "        distance = torch.norm(embed_a - embed_b, p=2, dim=1)\n",
    "\n",
    "        # Pérdida contrastiva\n",
    "        loss = label * distance.pow(2) + (1 - label) * F.relu(self.margin - distance).pow(2)\n",
    "        return loss.mean()\n",
    "\n",
    "# loss_fn = torch.nn.BCELoss()\n",
    "loss_fn = ContrastiveLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.comment_text_1 = dataframe.comment_text_1\n",
    "        self.comment_text_2 = dataframe.comment_text_2\n",
    "        self.targets = self.data.list\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment_text_1 = str(self.comment_text_1[index])\n",
    "        comment_text_1 = \" \".join(comment_text_1.split())\n",
    "        comment_text_2 = str(self.comment_text_2[index])\n",
    "        comment_text_2 = \" \".join(comment_text_2.split())\n",
    "        inputs0 = self.tokenizer(comment_text_1, \n",
    "                                comment_text_2, \n",
    "                                max_length=self.max_len,\n",
    "                                padding=\"max_length\",\n",
    "                                truncation=True,\n",
    "                                # truncation=\"only_second\",\n",
    "                                # truncation=\"only_first\",\n",
    "                                # truncation=\"longest_first\",\n",
    "                                return_overflowing_tokens=False,\n",
    "                                return_token_type_ids=True,)\n",
    "                                # return_overflowing_tokens=True)\n",
    "                                # return_overflowing_tokens=False)\n",
    "        inputs1 = self.tokenizer(comment_text_1,\n",
    "                                max_length=self.max_len,\n",
    "                                padding=\"max_length\",\n",
    "                                truncation=True,\n",
    "                                return_overflowing_tokens=False,\n",
    "                                return_token_type_ids=True,)\n",
    "        return {\n",
    "            'ids_0': torch.tensor(inputs0.input_ids, dtype=torch.long),\n",
    "            'mask_0': torch.tensor(inputs0.attention_mask, dtype=torch.long),\n",
    "            'token_type_ids_0': torch.tensor(inputs0.token_type_ids, dtype=torch.long),\n",
    "            'ids_1': torch.tensor(inputs1.input_ids, dtype=torch.long),\n",
    "            'mask_1': torch.tensor(inputs1.attention_mask, dtype=torch.long),\n",
    "            'token_type_ids_1': torch.tensor(inputs1.token_type_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(self.targets[index], dtype=torch.long)\n",
    "          }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia el dataset\n",
    "train_dataset = CustomDataset(dataframe=train, tokenizer=tokenizer, max_len=512)\n",
    "val_dataset = CustomDataset(dataframe=val, tokenizer=tokenizer, max_len=512)\n",
    "test_dataset = CustomDataset(dataframe=test, tokenizer=tokenizer, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_at_1(targets, preds):\n",
    "    \"\"\"\n",
    "    Calculates the C@1 metric:\n",
    "    - Non-answers (predictions marked as -1) are given a score of 0.5.\n",
    "    - Remaining cases are scored based on accuracy.\n",
    "    \n",
    "    Parameters:\n",
    "        targets (np.array): Ground truth labels.\n",
    "        preds (np.array): Predictions, where -1 indicates a non-answer.\n",
    "    \n",
    "    Returns:\n",
    "        float: C@1 metric.\n",
    "    \"\"\"\n",
    "    correct = (targets == preds)  # Boolean array for correct predictions\n",
    "    unanswered = preds == -1     # Boolean array for non-answers\n",
    "    \n",
    "    num_correct = correct.sum()\n",
    "    num_total = len(targets)\n",
    "    num_unanswered = unanswered.sum()\n",
    "    \n",
    "    return (num_correct + num_unanswered * 0.5) / num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, brier_score_loss, fbeta_score\n",
    "import numpy as np\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train_epoch(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(loader):\n",
    "        print(f\"Batch {i+1}/{len(loader)}\")\n",
    "        labels = batch['labels'].unsqueeze(1).to(device).float()\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        embed_a, embed_b = model(**batch)\n",
    "\n",
    "        loss = loss_fn(embed_a, embed_b, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Función de evaluación\n",
    "def evaluate(model, loader, prototype_human, device):\n",
    "    model.eval()\n",
    "    preds, targets, probabilities = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            labels = batch['labels'].unsqueeze(1).float()\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            embed_a, embed_b = model(**batch)\n",
    "            similarity_a_human = torch.cosine_similarity(embed_a, prototype_human.unsqueeze(0), dim=1)\n",
    "            similarity_b_human = torch.cosine_similarity(embed_b, prototype_human.unsqueeze(0), dim=1)\n",
    "            predictions = (similarity_a_human <= similarity_b_human).int()  # 1 for sim_b > sim_a, else 0\n",
    "            prob = similarity_b_human - similarity_a_human\n",
    "            preds.extend(predictions.cpu().numpy())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "            probabilities.extend(prob.cpu().numpy())\n",
    "    \n",
    "    targets = np.array(targets).flatten()\n",
    "    preds = np.array(preds).flatten()\n",
    "    probabilities = np.array(probabilities).flatten()\n",
    "\n",
    "\n",
    "    # Calculate metrics\n",
    "    roc_auc = roc_auc_score(targets, probabilities)\n",
    "    brier = brier_score_loss(targets, probabilities)\n",
    "    f1 = f1_score(targets, preds)\n",
    "    f05u = fbeta_score(targets, preds, beta=0.5)\n",
    "    c1 = c_at_1(targets, preds)\n",
    "    mean = np.mean([roc_auc, brier, c1, f1, f05u])\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(targets, preds),\n",
    "        \"roc-auc\": roc_auc,\n",
    "        \"brier\": brier,\n",
    "        \"c@1\": c1,\n",
    "        \"f1\": f1,\n",
    "        \"f05u\": f05u,\n",
    "        \"mean\": mean,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc  # Para recolección de basura\n",
    "\n",
    "human_text_tuples = [(text, text, 0) for text in train['text_human']]\n",
    "human_df = pd.DataFrame(human_text_tuples, columns=['comment_text_1', 'comment_text_2', 'list'])\n",
    "human_dataset = CustomDataset(dataframe=human_df, tokenizer=tokenizer, max_len=512)\n",
    "human_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "human_embeddings = []\n",
    "for i, batch in enumerate(human_loader):\n",
    "    print(f\"Batch {i+1}/{len(human_loader)}\")\n",
    "    labels = batch['labels'].unsqueeze(1).to(device).float()\n",
    "    batch = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "    embed_a, _ = model(**batch)\n",
    "    human_embeddings.extend(embed_a.cpu())\n",
    "\n",
    "stacked = torch.stack(human_embeddings)\n",
    "prototype_human = stacked.mean(dim=0).to(device)\n",
    "\n",
    "\n",
    "del human_embeddings, batch, embed_a, labels\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_metrics\": [],\n",
    "    \"val_metrics\": []\n",
    "}\n",
    "\n",
    "save_path = f\"models/models_contrastive/fine_tuned_model_{EPOCHS}_epochs_{LEARNING_RATE}_lr_{LAYERS_TO_TRAIN}_layers_{BATCH_SIZE}_batch_size\"\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Starting Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(\"* Training\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "\n",
    "    print(\"* Saving model\")\n",
    "    _epoch_save_path = f\"{save_path}_checkoint_{epoch + 1}.pth\"\n",
    "    torch.save(model, _epoch_save_path)\n",
    "\n",
    "    print(\"* Calculating metrics for training\")\n",
    "    train_metrics = evaluate(model, train_loader, prototype_human, device)\n",
    "    print(\"* Calculating metrics for validation\")\n",
    "    val_metrics = evaluate(model, val_loader, prototype_human, device)\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_metrics\"].append(train_metrics)\n",
    "    history[\"val_metrics\"].append(val_metrics)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(\"Train Metrics:\")\n",
    "    for metric_name, value in train_metrics.items():\n",
    "        print(f\"  {metric_name}: {value:.4f}\")\n",
    "    print(\"Validation Metrics:\")\n",
    "    for metric_name, value in val_metrics.items():\n",
    "        print(f\"  {metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación final en el conjunto de prueba\n",
    "test_accuracy = evaluate(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
