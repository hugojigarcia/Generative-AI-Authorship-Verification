{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sL8s_6Uk5YNS"
   },
   "source": [
    "Integrantes del equipo:\n",
    "- Alonso Cañas Rico\n",
    "- Hugo Jiménez García"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNr5zMgc5YNV"
   },
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1874,
     "status": "ok",
     "timestamp": 1731435422581,
     "user": {
      "displayName": "Hugo Jiménez García",
      "userId": "06485726344686732927"
     },
     "user_tz": -60
    },
    "id": "aSzipZl95f4b",
    "outputId": "17707c3a-6686-4086-bc51-1416ed670c3d"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1731435422582,
     "user": {
      "displayName": "Hugo Jiménez García",
      "userId": "06485726344686732927"
     },
     "user_tz": -60
    },
    "id": "GKn1bmaa5YNW"
   },
   "outputs": [],
   "source": [
    "ai_generated_path = \"/content/drive/MyDrive/HUGO/Master/NLP/pan24-generative-authorship-news/machines\"\n",
    "human_path = \"/content/drive/MyDrive/HUGO/Master/NLP/pan24-generative-authorship-news/human.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWXWzySt5YNW"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1731435422582,
     "user": {
      "displayName": "Hugo Jiménez García",
      "userId": "06485726344686732927"
     },
     "user_tz": -60
    },
    "id": "qeUPp2_A5YNX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50rub4dv5YNX"
   },
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 691,
     "status": "ok",
     "timestamp": 1731435423264,
     "user": {
      "displayName": "Hugo Jiménez García",
      "userId": "06485726344686732927"
     },
     "user_tz": -60
    },
    "id": "_gOQsxrH5YNX",
    "outputId": "87ee1c86-3d8f-4943-e9c8-a53dd9489609"
   },
   "outputs": [],
   "source": [
    "model, id, text = [], [], []\n",
    "\n",
    "# Loop through every file in the directory\n",
    "for filename in os.listdir(ai_generated_path):\n",
    "    # Check if the file is a JSONL file\n",
    "    if filename.endswith('.jsonl'):\n",
    "        filepath = os.path.join(ai_generated_path, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as jsonl_file:\n",
    "            for line in jsonl_file:\n",
    "                # Each line is a separate JSON object\n",
    "                data = json.loads(line)\n",
    "                model.append(filename)\n",
    "                id.append(data['id'])\n",
    "                text.append(data['text'])\n",
    "\n",
    "df_generated = pd.DataFrame({'model': model, 'id': id, 'text': text, 'ai_generated': 1})\n",
    "df_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1731435423265,
     "user": {
      "displayName": "Hugo Jiménez García",
      "userId": "06485726344686732927"
     },
     "user_tz": -60
    },
    "id": "N5mdYLgR5YNa",
    "outputId": "35f45c67-b70f-4884-9122-400164f7324d"
   },
   "outputs": [],
   "source": [
    "id, text = [], []\n",
    "\n",
    "with open(human_path, 'r', encoding='utf-8') as jsonl_file:\n",
    "    for line in jsonl_file:\n",
    "        # Each line is a separate JSON object\n",
    "        data = json.loads(line)\n",
    "        id.append(data['id'])\n",
    "        text.append(data['text'])\n",
    "\n",
    "df_human = pd.DataFrame({'model': 'Human', 'id': id, 'text': text, 'ai_generated': 0})\n",
    "df_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1731435423554,
     "user": {
      "displayName": "Hugo Jiménez García",
      "userId": "06485726344686732927"
     },
     "user_tz": -60
    },
    "id": "SrHvBgX25YNb",
    "outputId": "a6d62806-0f0c-4cf6-85b6-24f8c5f39e91"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df_generated, df_human])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1731435423554,
     "user": {
      "displayName": "Hugo Jiménez García",
      "userId": "06485726344686732927"
     },
     "user_tz": -60
    },
    "id": "Wzg1fFY25YNb",
    "outputId": "5d600403-d56b-4759-b1c6-07bfc9392735"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.2\n",
    "val_size = 0.1\n",
    "_adjusted_val_size = val_size / (1 - test_size)\n",
    "\n",
    "# Assume X is your features and y is your target variable\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(df.drop(columns=['ai_generated']), df['ai_generated'], test_size=test_size, stratify=df['ai_generated'], shuffle=True, random_state=1337)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=_adjusted_val_size, stratify=y_train_val, shuffle=True, random_state=1337)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "X_val.reset_index(drop=True, inplace=True)\n",
    "y_val.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "# Print the dimensions\n",
    "print(f\"X_train shape: {X_train.shape} / AI generated count: {y_train.value_counts()[1]} - Human count: {y_train.value_counts()[0]}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape} / AI generated count: {y_val.value_counts()[1]} - Human count: {y_val.value_counts()[0]}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape} / AI generated count: {y_test.value_counts()[1]} - Human count: {y_test.value_counts()[0]}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pG82NuUA5YNc"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1731435423898,
     "user": {
      "displayName": "Hugo Jiménez García",
      "userId": "06485726344686732927"
     },
     "user_tz": -60
    },
    "id": "Bz17SJ3D6WIm",
    "outputId": "bfc6f612-e730-4bd5-be77-679cb9880583"
   },
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"Lau123/distilbert-base-uncased-detect_ai_generated_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 593350,
     "status": "ok",
     "timestamp": 1731436017239,
     "user": {
      "displayName": "Hugo Jiménez García",
      "userId": "06485726344686732927"
     },
     "user_tz": -60
    },
    "id": "rYPeWeZYlMbb",
    "outputId": "726e3984-267b-412d-d2aa-f5667e049313"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define un dataset personalizado compatible con PyTorch\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Tokenizador y modelo\n",
    "model_name = \"Lau123/distilbert-base-uncased-detect_ai_generated_text\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset = CustomDataset(X_train['text'], y_train, tokenizer)\n",
    "val_dataset = CustomDataset(X_val['text'], y_val, tokenizer)\n",
    "test_dataset = CustomDataset(X_test['text'], y_test, tokenizer)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "\n",
    "# Configuración del dispositivo y optimizador\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Función de evaluación\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            preds.extend(torch.argmax(outputs.logits, dim=-1).cpu().numpy())\n",
    "            targets.extend(batch[\"labels\"].cpu().numpy())\n",
    "    return accuracy_score(targets, preds)\n",
    "\n",
    "# Entrenamiento y validación\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_accuracy = evaluate(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Guardar el modelo\n",
    "model.save_pretrained(\"fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_model\")\n",
    "\n",
    "# Evaluación final en el conjunto de prueba\n",
    "test_accuracy = evaluate(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1731436097921,
     "user": {
      "displayName": "Hugo Jiménez García",
      "userId": "06485726344686732927"
     },
     "user_tz": -60
    },
    "id": "Z5AKBdx1oiJT",
    "outputId": "78fc0d22-d949-4a11-b1b5-ef84f14e0d3c"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVvH53EFoUdu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Asumimos que el modelo y el tokenizador ya están cargados\n",
    "# model y tokenizer ya están definidos previamente\n",
    "\n",
    "# Filtrar por label en X_test\n",
    "label_1_texts = X_test.loc[y_test == 1, 'text'].reset_index(drop=True)\n",
    "label_0_texts = X_test.loc[y_test == 0, 'text'].reset_index(drop=True)\n",
    "\n",
    "# Crear todas las combinaciones entre label_1 y label_0\n",
    "combinations = list(product(label_1_texts, label_0_texts))\n",
    "\n",
    "# Función para obtener predicciones por lotes\n",
    "def batch_predict(model, tokenizer, text_pairs, device, batch_size=16):\n",
    "    predictions = []\n",
    "    for i in range(0, len(text_pairs), batch_size):\n",
    "        batch = text_pairs[i:i + batch_size]\n",
    "        texts1, texts2 = zip(*batch)\n",
    "\n",
    "        # Tokenización por lotes\n",
    "        inputs1 = tokenizer(list(texts1), return_tensors=\"pt\", max_length=128,\n",
    "                            padding=\"max_length\", truncation=True).to(device)\n",
    "        inputs2 = tokenizer(list(texts2), return_tensors=\"pt\", max_length=128,\n",
    "                            padding=\"max_length\", truncation=True).to(device)\n",
    "\n",
    "        # Obtener logits\n",
    "        with torch.no_grad():\n",
    "            logits1 = model(**inputs1).logits\n",
    "            logits2 = model(**inputs2).logits\n",
    "\n",
    "        # Calcular probabilidades\n",
    "        probs1 = torch.softmax(logits1, dim=-1).cpu().numpy()\n",
    "        probs2 = torch.softmax(logits2, dim=-1).cpu().numpy()\n",
    "\n",
    "        # Evaluar las probabilidades para determinar cuál texto es label_0 y cuál label_1\n",
    "        for idx in range(len(batch)):\n",
    "            predicted_label_0 = texts1[idx] if probs1[idx][0] > probs2[idx][0] else texts2[idx]\n",
    "            predicted_label_1 = texts2[idx] if probs1[idx][0] > probs2[idx][0] else texts1[idx]\n",
    "\n",
    "            predictions.append({\n",
    "                \"text1\": texts1[idx],\n",
    "                \"text2\": texts2[idx],\n",
    "                \"predicted_label_0\": predicted_label_0,\n",
    "                \"predicted_label_1\": predicted_label_1\n",
    "            })\n",
    "    return predictions\n",
    "\n",
    "# Predecir todas las combinaciones en lotes\n",
    "batch_size = 256\n",
    "results = batch_predict(model, tokenizer, combinations, device, batch_size)\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Evaluar la precisión\n",
    "correct_predictions = 0\n",
    "for _, row in results_df.iterrows():\n",
    "    # Recuperar el índice de los textos en X_test\n",
    "    true_label_0 = X_test.loc[X_test['text'] == row['predicted_label_0']].index[0]\n",
    "    true_label_1 = X_test.loc[X_test['text'] == row['predicted_label_1']].index[0]\n",
    "\n",
    "    # Verificar si las predicciones coinciden con las etiquetas reales\n",
    "    if y_test[true_label_0] == 0 and y_test[true_label_1] == 1:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / len(results_df)\n",
    "print(f\"Pairwise Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYz_AxrilK5i"
   },
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1731434371354,
     "user": {
      "displayName": "Hugo Jiménez García",
      "userId": "06485726344686732927"
     },
     "user_tz": -60
    },
    "id": "Na0-EkF5r8K4"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "# # Replace 'your_api_key_here' with your actual Hugging Face API key\n",
    "# api_key = \"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\"\n",
    "\n",
    "# # Login programmatically\n",
    "# HfFolder.save_token(api_key)\n",
    "\n",
    "# # Now you can use the HfApi with the token\n",
    "# api = HfApi()\n",
    "# user_info = api.whoami()\n",
    "# print(\"Logged in as:\", user_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1731434371354,
     "user": {
      "displayName": "Hugo Jiménez García",
      "userId": "06485726344686732927"
     },
     "user_tz": -60
    },
    "id": "J9tgx1gu-SMA"
   },
   "outputs": [],
   "source": [
    "# # Use a pipeline as a high-level helper\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"text-classification\", model=\"meta-llama/Llama-3.2-1B\", device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 735,
     "status": "ok",
     "timestamp": 1731434372081,
     "user": {
      "displayName": "Hugo Jiménez García",
      "userId": "06485726344686732927"
     },
     "user_tz": -60
    },
    "id": "pFfWN-BZ6l_e",
    "outputId": "246c33f0-b5c8-454b-c15e-0367496041d9"
   },
   "outputs": [],
   "source": [
    "def prediction(index, X, y, pipe):\n",
    "  x = X.iloc[index].text[:512]\n",
    "  y = y[index]\n",
    "  y_predict = pipe(x)\n",
    "  print(f\"Label: {y}\")\n",
    "  print(f\"Prediction: {y_predict}\")\n",
    "  print()\n",
    "  return y_predict\n",
    "\n",
    "prediction(0, X_train, y_train, pipe)\n",
    "prediction(3, X_train, y_train, pipe)\n",
    "prediction(5, X_train, y_train, pipe)\n",
    "prediction(78, X_train, y_train, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-uX5MpRvMHt"
   },
   "outputs": [],
   "source": [
    "aciertos, errores = 0, 0\n",
    "for i in range(len(X_test)):\n",
    "  y_predict = prediction(i, X_test, y_test, pipe)\n",
    "  if y_predict[0]['label'] == 'LABEL_1':\n",
    "    y_predict = 1\n",
    "  else:\n",
    "    y_predict = 0\n",
    "  if y_predict == y_test[i]:\n",
    "    aciertos += 1\n",
    "  else:\n",
    "    errores += 1\n",
    "print(f\"Aciertos: {aciertos}\")\n",
    "print(f\"Errores: {errores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "error",
     "timestamp": 1731434769111,
     "user": {
      "displayName": "Hugo Jiménez García",
      "userId": "06485726344686732927"
     },
     "user_tz": -60
    },
    "id": "QndwqbmeUlk3",
    "outputId": "4094149a-f616-42d3-b279-a9ffdb9e8270"
   },
   "outputs": [],
   "source": [
    "y_predict[0]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
